{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from fbprophet import Prophet\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='partha1989', api_key='Y2Yzydnp97ZO3Qoi8f1y')\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from fbprophet.diagnostics import cross_validation, performance_metrics\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from geolib import geohash\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***Function to create line chart***\n",
    "def plotlyline(x_val,y_val,types,name):\n",
    "    line = go.Scatter(x = x_val,y = y_val,mode = types,name = name)\n",
    "    return line\n",
    "\n",
    "# ***Function to create Bar chart***\n",
    "def plotlybar(x_val,y_val):\n",
    "    bar = go.Bar(x=x_val,y=y_val,text=y_val,textposition = 'auto',name='bars')\n",
    "    return bar\n",
    "\n",
    "# ***Function to create Geo map***\n",
    "def plotlygeo(lat,long):\n",
    "    data = [go.Scattergeo(lat = lat ,lon = long)]\n",
    "    return data\n",
    "\n",
    "# ***Function for Box plot***\n",
    "def plotlybox(y):\n",
    "    data = [go.Box(y=y,boxpoints='all',jitter=0.3,pointpos=-1.8)]\n",
    "    return data\n",
    "\n",
    "# ***Function to create dummy dates and assign Index to it***\n",
    "def dummydate(startdate, index_end , index_start):\n",
    "    date_list = pd.DataFrame()\n",
    "    date_list[\"Date\"] = [(startdate + timedelta(days=x)).strftime(\"%Y-%m-%d\") for x in range(0, index_end)]\n",
    "    date_list[\"Date\"] =pd.to_datetime(date_list.Date, format='%Y-%m-%d').dt.date\n",
    "    date_list = date_list.sort_values(by='Date')\n",
    "    date_list['day'] = np.arange(date_list.shape[0]) + index_start\n",
    "    return date_list\n",
    "\n",
    "def dataprep(startdate, index, dataframe):\n",
    "    ## Dummy start date for forecasting - Replace with actual dates\n",
    "    startdate = datetime.strptime(startdate, '%Y-%m-%d')\n",
    "    ## Creating dummy date with index\n",
    "    datelist = dummydate(startdate, index, dataframe[\"day\"].min() )\n",
    "    dump_fin = pd.merge(dataframe, datelist, how='inner')\n",
    "    dump_fin[\"Datetime\"] = dump_fin[\"Date\"].astype(str)+ \" \" + dump_fin[\"timestamp\"].astype(str)\n",
    "    dump_fin[\"Datetime\"] = pd.to_datetime(dump_fin[\"Datetime\"])\n",
    "    ## Creating dataset with required columns\n",
    "    demand = dump_fin.filter(['geohash6','Datetime','demand'], axis=1)\n",
    "    demand[\"Datetime\"] = pd.to_datetime(demand[\"Datetime\"])\n",
    "    return demand\n",
    "\n",
    "# ***Function to convert geohash to Lat & Long***\n",
    "def geohashlatlong(dataframe):\n",
    "    geohash6 = dataframe[\"geohash6\"].unique()\n",
    "    geo_data = pd.DataFrame()\n",
    "    for geo in geohash6:\n",
    "        lat = geohash.decode(geo)[0]\n",
    "        long = geohash.decode(geo)[1]\n",
    "        temp = pd.DataFrame({'Geo': [geo],'lat': [lat], 'long': [long]})\n",
    "        geo_data = geo_data.append(temp)\n",
    "    return geo_data\n",
    "\n",
    "# ***Create dataframe for geohash (by precision)***\n",
    "def geohash5fn(dataframe,precision):\n",
    "    geohash_5 = dataframe.copy()\n",
    "    geohash_5[\"geohash5\"] = dataframe[\"geohash6\"].str[:precision]\n",
    "    geohash_5 = geohash_5.groupby(['Datetime','geohash5'])['demand'].sum().reset_index()\n",
    "    return geohash_5\n",
    "\n",
    "\n",
    "# ***Create prediction and save model***\n",
    "def predictiongeo(train,test,remove_geohash,export_model):\n",
    "    \n",
    "    daystopredict = (test[\"Datetime\"].dt.date.max() - train[\"Datetime\"].dt.date.max()).days\n",
    "    period = round(((24 *60)/15)* int(daystopredict+6))\n",
    "    train= train[~train[\"geohash6\"].isin (remove_geohash)]\n",
    "    train[\"hour\"] =  train[\"Datetime\"].dt.hour\n",
    "    df = train.copy()\n",
    "    df = df.rename(index=str, columns={\"Datetime\": \"ds\", \"demand\": \"y\"})\n",
    "    #df[\"on_season\"] = np.where(((df['ds'] >  (df[\"ds\"].min() + timedelta(days=31)).strftime(\"%Y-%m-%d\")) &(df['ds'] <  (df[\"ds\"].min() + timedelta(days=58)).strftime(\"%Y-%m-%d\")) ),1,0)\n",
    "    #df[\"cap\"] = maxi\n",
    "    #df[\"floor\"] = mini\n",
    "    #hash5 = ('qp02yc', 'qp02yf', 'qp02yv', 'qp02yy', 'qp02yz')\n",
    "    hash5 = df.geohash6.unique()\n",
    "    \n",
    "    test_val = test[test[\"geohash6\"].isin(hash5)].copy()\n",
    "    test_val = test_val.rename(index=str, columns={\"Datetime\": \"ds\", \"demand\": \"y\"})\n",
    "    test_val[\"hour\"] = test_val[\"ds\"].dt.hour\n",
    "    max_test_ds = test_val[\"ds\"].max()\n",
    "    #test_val[\"on_season\"]=1\n",
    "    fsct_test = pd.DataFrame()\n",
    "\n",
    "\n",
    "    fsct_fin = pd.DataFrame()\n",
    "    # Prediction by Geohash\n",
    "    for geo in hash5:\n",
    "        geo_df = df[df['geohash6'] == geo].copy()\n",
    "        geo_test = test_val[test_val['geohash6'] == geo].copy()\n",
    "        my_model = Prophet(interval_width=0.95,yearly_seasonality=False)\n",
    "        my_model.add_regressor('hour')\n",
    "#         my_model.add_regressor('sum_neighbours')\n",
    "#         my_model.add_regressor('mean_neighbours')\n",
    "#         my_model.add_regressor('min_neighbours')\n",
    "#         my_model.add_regressor('max_neighbours')\n",
    "#         my_model.add_regressor('std_neighbours')\n",
    "        my_model.fit(geo_df)\n",
    "        future_dates = my_model.make_future_dataframe( periods = period, freq = '15min' )\n",
    "        future_dates[\"hour\"] =  future_dates[\"ds\"].dt.hour\n",
    "        #future_dates = future_dates[future_dates[\"ds\"] > max_test_ds]\n",
    "        #future_dates[\"on_season\"] = 1\n",
    "        #future_dates[\"cap\"] = maxi\n",
    "        #future_dates[\"floor\"] = mini\n",
    "        forecast = my_model.predict(future_dates)\n",
    "        forecast[\"geohash\"] = geo\n",
    "        fsct_fin = fsct_fin.append(forecast)\n",
    "        \n",
    "        if (geo_test.shape[0] > 0):\n",
    "            forecast_test = my_model.predict(geo_test)\n",
    "            forecast_test[\"geohash\"] = geo\n",
    "            fsct_test = fsct_test.append(forecast_test)\n",
    "\n",
    "        # Saving model output to the file in the current working directory (cannot save in DB) and resue if needed\n",
    "        if (export_model == True):\n",
    "            pkl_filename = geo +\"_model.pkl\"\n",
    "            with open(pkl_filename, 'wb') as file:  \n",
    "                pickle.dump(my_model, file)\n",
    "    return fsct_fin , fsct_test\n",
    "\n",
    "# ***Replace dataframe with negative values to 0***\n",
    "def replacenegative(dataframe):\n",
    "    num = dataframe._get_numeric_data()\n",
    "    num[num < 0] = 0\n",
    "    return dataframe\n",
    "\n",
    "def rmse(actual,predicted):\n",
    "    rms = sqrt(mean_squared_error(actual, predicted))\n",
    "    return rms\n",
    "\n",
    "def predictingtest(train,test,remove_geohash):\n",
    "    train= train[~train[\"geohash6\"].isin (remove_geohash)]\n",
    "    geohash = train[\"geohash6\"].unique()\n",
    "    test_val = test[test[\"geohash6\"].isin(geohash)].copy()\n",
    "    test_val = test_val.rename(index=str, columns={\"Datetime\": \"ds\", \"demand\": \"y\"})\n",
    "    test_val[\"hour\"] = test_val[\"ds\"].dt.hour\n",
    "    #test_val[\"on_season\"]=1\n",
    "    hash_val  = test_val[\"geohash6\"].unique()\n",
    "    fsct_fin = pd.DataFrame()\n",
    "    for geo in hash_val:\n",
    "        geo_df = test_val[test_val['geohash6'] == geo]\n",
    "        pkl_filename = geo +\"_model.pkl\"\n",
    "        with open(pkl_filename, 'rb') as fin:\n",
    "            m2 = pickle.load(fin)\n",
    "        fcst = m2.predict(geo_df)\n",
    "        fcst[\"geohash\"] = geo\n",
    "        fsct_fin = fsct_fin.append(fcst)\n",
    "    return fsct_fin\n",
    "\n",
    "def mape(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def createcsv(dataframe,filter_days_start,filter_days_end,filename):\n",
    "    dataframe[(dataframe[\"day\"] >= int(filter_days_start)) & (dataframe[\"day\"] <= int(filter_days_end))].to_csv(filename,index = None, header=True)\n",
    "    \n",
    "def readcsv():\n",
    "    data = pd.read_csv(\"training_final.csv\")\n",
    "    return data\n",
    "\n",
    "def fillnan(dataframe, column):\n",
    "    dataframe[column] = dataframe[column].fillna(0)\n",
    "    return dataframe\n",
    "\n",
    "def geohashneighbor(geohash6):\n",
    "    neighbour_df = pd.DataFrame()\n",
    "    for geo in geohash6:\n",
    "        neighbour_dict = pd.DataFrame()\n",
    "        neighbour = geohash.neighbours(geo)\n",
    "        neighbour_dict[\"neighbour\"] = list(neighbour)\n",
    "        neighbour_dict[\"parent\"] = geo\n",
    "        neighbour_df = neighbour_df.append(neighbour_dict)\n",
    "    return neighbour_df\n",
    "\n",
    "def geohashneighbours(dataframe):\n",
    "    neighbour =  geohashneighbor(dataframe[\"geohash6\"].unique())\n",
    "    geohash = dataframe[\"geohash6\"].unique()\n",
    "    dump = pd.DataFrame()\n",
    "    for geo in geohash:\n",
    "        temp_dataframe = dataframe[dataframe[\"geohash6\"] == geo].copy()\n",
    "        temp_neighbour = neighbour[neighbour[\"parent\"] == geo][\"neighbour\"]\n",
    "        temp_data = dataframe[dataframe[\"geohash6\"].isin (temp_neighbour)].copy()\n",
    "        neighbour_stats =  temp_data.groupby(temp_data[\"Datetime\"]).agg({'demand' :[sum,np.mean,min,max,np.std]}).reset_index()\n",
    "        neighbour_stats.columns = [\"Datetime\",\"sum_neighbours\",\"mean_neighbours\",\"min_neighbours\",\"max_neighbours\",\"std_neighbours\"]\n",
    "        temp_dump = pd.merge(temp_dataframe, neighbour_stats, how='left')\n",
    "        dump = dump.append(temp_dump)\n",
    "    return dump\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Assuming that training and test file has same column names. Day is continuous value from Train file (any value above 61) .\n",
    "# Prophet assumes 0 for missing dates ***\n",
    "\n",
    "# *** I have created Train & Test csv file to test  my code ***\n",
    "\n",
    "# dummy = readcsv()\n",
    "# createcsv(dummy ,1,56,\"training.csv\")\n",
    "# createcsv(dummy ,57,61,\"test.csv\")\n",
    "\n",
    "# *** Load training file***\n",
    "dump_train = pd.read_csv(\"training.csv\")\n",
    "\n",
    "# *** Load test file - replace the proper file name***\n",
    "\n",
    "dump_test = pd.read_csv(\"test.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "## Time Series forecasting\n",
    "\n",
    "### Time series requires date for forecasting. So I have used dummy dates to extract seasonality & trend. I would request grab team to subsititute exact start date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***Enter start date for forecasting - Replace with actual dates (Please enter start date for training dataset)***\n",
    "\n",
    "start_date = '2019-04-01'\n",
    "\n",
    "train = dataprep( start_date, dump_train[\"day\"].max(),dump_train)\n",
    "\n",
    "# *** You can add exact start date of test data set ***\n",
    "\n",
    "start_date_test =  '2019-05-18'\n",
    "\n",
    "test = dataprep( start_date_test, ((dump_test[\"day\"].max() - dump_test[\"day\"].min()).astype(int))+1, dump_test)\n",
    "\n",
    "# *** Continue running below code *** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Extracting features from nearby neighbours - Not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train = geohashneighbours(train)\n",
    "# test = geohashneighbours(test)\n",
    "# # ***Check null values***\n",
    "# print(train.isnull().sum().sum())\n",
    "# print(test.isnull().sum().sum())\n",
    "# # *** Replace Null with 0 ***\n",
    "# train = train.fillna(0)\n",
    "# test = test.fillna(0)\n",
    "# # ***Check null values***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train.isnull().sum().sum())\n",
    "print(test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~partha1989/16.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_demand = dump_train.groupby('day')['demand'].sum().reset_index()\n",
    "layout = dict(title = 'Box Plot Demand',\n",
    "               xaxis = dict(title = 'Demand')\n",
    "              )\n",
    "fig = dict(data = plotlybox(pd.Series(plot_demand[\"demand\"])), layout=layout)\n",
    "py.iplot(fig, filename='Demand -boxplot')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~partha1989/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_demand = dump_train.groupby('day')['demand'].sum().reset_index()\n",
    "layout = dict(title = 'Daywise Demand',\n",
    "              xaxis = dict(title = 'Day'),\n",
    "              yaxis = dict(title = 'Demand'),\n",
    "              )\n",
    "fig = dict(data = [plotlyline(day_demand[\"day\"],day_demand[\"demand\"],\"lines\",\"demand\")], layout=layout)\n",
    "py.iplot(fig, filename='Demand - Day wise')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~partha1989/8.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geohash_demand = train.groupby(train[\"Datetime\"].dt.hour)['demand'].sum().reset_index()\n",
    "layout = dict(title = 'Geohash Demand',\n",
    "              xaxis = dict(title = 'Hour'),\n",
    "              yaxis = dict(title = 'Demand'),\n",
    "              )\n",
    "fig = dict(data = [plotlybar(geohash_demand[\"Datetime\"],round(geohash_demand[\"demand\"]))], layout=layout)\n",
    "py.iplot(fig, filename='Demand - Hour wise')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seems bit fishy to have less demand on Friday where it usually peaks (from my experience in Careem - Last working day of week. By giving actual datesthings might change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~partha1989/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorter = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "sorterIndex = dict(zip(sorter,range(len(sorter))))\n",
    "daywise_demand = train.groupby(train[\"Datetime\"].dt.strftime(\"%A\"))['demand'].sum().reset_index()\n",
    "\n",
    "daywise_demand['Day_id'] = daywise_demand.index\n",
    "\n",
    "daywise_demand['Day_id_rnk'] = daywise_demand['Datetime'].map(sorterIndex)\n",
    "\n",
    "daywise_demand.sort_values('Day_id_rnk', inplace=True)\n",
    "\n",
    "\n",
    "layout = dict(title = 'Geohash Demand',\n",
    "              xaxis = dict(title = 'Day Name'),\n",
    "              yaxis = dict(title = 'Demand'),\n",
    "              )\n",
    "\n",
    "fig = dict(data = [plotlybar(daywise_demand[\"Datetime\"],round(daywise_demand[\"demand\"]))], layout=layout)\n",
    "py.iplot(fig, filename='Demand - Day wise')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Geo hash to lat and long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_data = geohashlatlong(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Plotting Lat and Long on map, all the location given in dataset lies in Indian ocean :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~partha1989/12.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout = dict(\n",
    "        title = 'Latitude & Longititude Maps', \n",
    "        geo = dict(\n",
    "            scope='asia',\n",
    "            showland = True,\n",
    "            landcolor = \"rgb(250, 250, 250)\",\n",
    "            subunitcolor = \"rgb(217, 217, 217)\",\n",
    "            countrycolor = \"rgb(217, 217, 217)\",\n",
    "            countrywidth = 0.5,\n",
    "            subunitwidth = 0.5        \n",
    "        ),\n",
    "    )\n",
    "\n",
    "fig = go.Figure(data=plotlygeo(geo_data[\"lat\"],geo_data[\"long\"]),layout=layout )\n",
    "py.iplot(fig, filename='Latitude & Longitiude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3.845621e+06\n",
       "mean     1.042759e-01\n",
       "std      1.583243e-01\n",
       "min      3.092217e-09\n",
       "25%      1.855817e-02\n",
       "50%      5.006287e-02\n",
       "75%      1.198046e-01\n",
       "max      1.000000e+00\n",
       "Name: demand, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ***data is already normalized so outliers are handled***\n",
    "train[\"demand\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting\n",
    "\n",
    "### Prophet package handles missing timeseries effectively so I am not imputing it. \n",
    "\n",
    "### We are removing Geohash with less than 2 data points as Prophet needs min 2 or more values to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of Geohash with Data points less than 3 data points  is demand    1.430723\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "chk = train.groupby([\"geohash6\"])['demand'].count()\n",
    "chk = chk.to_frame()\n",
    "print(\"% of Geohash with Data points less than 3 data points  is\",(chk[chk[\"demand\"] <=2].count()/chk.shape[0])*100)\n",
    "remove_geohash = chk[chk[\"demand\"] <=2].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parthasarathysridharan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:68: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "/Users/parthasarathysridharan/anaconda3/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning:\n",
      "\n",
      "Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 17.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 10.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 22.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 6.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 11.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 12.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 23.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 16.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 6.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 13.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 17.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 19.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 3.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 4.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 11.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 17.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 3.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 8.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 14.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 3.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 3.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 9.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 4.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 7.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 7.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 9.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 3.\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 2.\n",
      "/Users/parthasarathysridharan/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning:\n",
      "\n",
      "Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 12.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 9.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 9.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 3.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 7.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 15.\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 2.\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 1.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 4.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 10.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 3.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 5.\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 1.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 2.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 10.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 1.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 1.\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 5.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 5.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 2.\n",
      "INFO:fbprophet:n_changepoints greater than number of observations.Using 2.\n"
     ]
    }
   ],
   "source": [
    "# ***Prediction Removing geohash less than 2 data points for prediction. Ignoring Geohash which is not part of training set\n",
    "# predictiongeo(training dataset, test dataset ,remove geohash less than 2 data points , whether to save model in\n",
    "#pickle file ) ***\n",
    "# Approx 3 hrs to run #\n",
    "\n",
    "\n",
    "fsct_train_mdl,fsct_test_mdl =  predictiongeo(train,test,remove_geohash,False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I am aggregating  actual and predicted values by Geohash so that it is easy to visualize however  final RMSE is actually average of Geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_geo_train = pd.merge(fsct_train_mdl,train, how='inner',left_on=['ds','geohash'],right_on=['Datetime','geohash6'])\n",
    "fs_geo_test = pd.merge(fsct_test_mdl,test, how='left',left_on=['ds','geohash'],right_on=['Datetime','geohash6'])\n",
    "\n",
    "\n",
    "fs_geo_test = fillnan(fs_geo_test ,\"demand\")\n",
    "\n",
    "fs_overall_train = fs_geo_train.groupby(['ds']).agg(\n",
    "    {\n",
    "         'yhat':sum,    \n",
    "         'yhat_lower': sum,  \n",
    "         'yhat_upper': sum,\n",
    "         'demand': sum  \n",
    "    }).reset_index()\n",
    "\n",
    "fs_overall_test = fs_geo_test.groupby(['ds']).agg(\n",
    "    {\n",
    "         'yhat':sum,    \n",
    "         'yhat_lower': sum,  \n",
    "         'yhat_upper': sum,\n",
    "         'demand': sum  \n",
    "    }).reset_index()\n",
    "\n",
    "# Replace negative values with 0\n",
    "fs_overall_train = replacenegative(fs_overall_train)\n",
    "fs_overall_test = replacenegative(fs_overall_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast Train Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~partha1989/14.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout = dict(title = 'Forecasting Train',\n",
    "              xaxis = dict(title = 'Day'),\n",
    "              yaxis = dict(title = 'Demand/Forecasting'),\n",
    "              )\n",
    "\n",
    "demand = plotlyline(fs_overall_train[\"ds\"],fs_overall_train[\"demand\"],\"lines\",\"demand\")\n",
    "yhat = plotlyline(fs_overall_train[\"ds\"],fs_overall_train[\"yhat\"],\"lines\",\"yhat\")\n",
    "yhat_lower = plotlyline(fs_overall_train[\"ds\"],fs_overall_train[\"yhat_lower\"],\"lines\",\"yhat_lower\")\n",
    "yhat_upper = plotlyline(fs_overall_train[\"ds\"],fs_overall_train[\"yhat_upper\"],\"lines\",\"yhat_upper\")\n",
    "data = [demand, yhat, yhat_lower, yhat_upper]\n",
    "\n",
    "\n",
    "fig = dict(data = data, layout=layout)\n",
    "py.iplot(fig, filename='Forecast plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~partha1989/14.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout = dict(title = 'Forecasting Train',\n",
    "              xaxis = dict(title = 'Day'),\n",
    "              yaxis = dict(title = 'Demand/Forecasting'),\n",
    "              )\n",
    "\n",
    "demand = plotlyline(fs_overall_train[\"ds\"],fs_overall_train[\"demand\"],\"lines\",\"demand\")\n",
    "yhat = plotlyline(fs_overall_train[\"ds\"],fs_overall_train[\"yhat\"],\"lines\",\"yhat\")\n",
    "data = [demand, yhat]\n",
    "\n",
    "\n",
    "fig = dict(data = data, layout=layout)\n",
    "py.iplot(fig, filename='Forecast plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Forecast Test Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~partha1989/14.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout = dict(title = 'Forecasting Test',\n",
    "              xaxis = dict(title = 'Day'),\n",
    "              yaxis = dict(title = 'Demand/Forecasting'),\n",
    "              )\n",
    "\n",
    "demand = plotlyline(fs_overall_test[\"ds\"],fs_overall_test[\"demand\"],\"lines\",\"demand\")\n",
    "yhat = plotlyline(fs_overall_test[\"ds\"],fs_overall_test[\"yhat\"],\"lines\",\"yhat\")\n",
    "yhat_lower = plotlyline(fs_overall_test[\"ds\"],fs_overall_test[\"yhat_lower\"],\"lines\",\"yhat_lower\")\n",
    "yhat_upper = plotlyline(fs_overall_test[\"ds\"],fs_overall_test[\"yhat_upper\"],\"lines\",\"yhat_upper\")\n",
    "data = [demand, yhat, yhat_lower, yhat_upper]\n",
    "\n",
    "\n",
    "fig = dict(data = data, layout=layout)\n",
    "py.iplot(fig, filename='Forecast plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~partha1989/14.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout = dict(title = 'Forecasting Test',\n",
    "              xaxis = dict(title = 'Day'),\n",
    "              yaxis = dict(title = 'Demand/Forecasting'),\n",
    "              )\n",
    "\n",
    "demand = plotlyline(fs_overall_test[\"ds\"],fs_overall_test[\"demand\"],\"lines\",\"demand\")\n",
    "yhat = plotlyline(fs_overall_test[\"ds\"],fs_overall_test[\"yhat\"],\"lines\",\"yhat\")\n",
    "data = [demand, yhat]\n",
    "\n",
    "\n",
    "fig = dict(data = data, layout=layout)\n",
    "py.iplot(fig, filename='Forecast plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset which was not used to train model showed RMSE of 0.06 compared to 0.05 in train dataset\n",
    "\n",
    "\n",
    "### RMSE (root mean squared error) averaged over all geohash6, 15-minute-bucket pairs.\n",
    "\n",
    "### If we know which city then we can add On/Off season, Holiday calendar, Weather to improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall RMSE of Train 0.04988658578580629\n",
      "Overall RMSE of Test 0.059597760594303786\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall RMSE of Train\",rmse(fs_geo_train[\"demand\"],fs_geo_train[\"yhat\"]))\n",
    "print(\"Overall RMSE of Test\",rmse(fs_geo_test[\"demand\"],fs_geo_test[\"yhat\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
